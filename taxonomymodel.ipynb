{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3979912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 17:53:14.161492: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-19 17:53:14.204514: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-19 17:53:14.362201: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-19 17:53:14.363100: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 17:53:15.133153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K \n",
    "from tf_crf_layer.layer import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc82ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Charge Dataset\n",
    "\n",
    "data = pd.read_csv('/home/pedro/Documents/JADS/DataConsul/updated_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3713ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Product Encoder\n",
    "\n",
    "def create_product_encoder(vocab_size, embedding_dim, lstm_units):\n",
    "    \n",
    "    input_tokens = Input(shape=(None,), dtype='int32', name='input_tokens')\n",
    "    \n",
    "    embedded_tokens = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_tokens)\n",
    "    \n",
    "    encoded_tokens = Bidirectional(LSTM(lstm_units, return_sequences=True))(embedded_tokens)\n",
    "    \n",
    "    return Model(input=input_tokens, outputs=encoded_tokens, name='product_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e39e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Category Encoder \n",
    "\n",
    "def create_category_encoder(category_count, embedding_dim, lstm_units): \n",
    "    \n",
    "    input_categories = Input(shape=(None,), dtype='int32', name='input_categories')\n",
    "    \n",
    "    embedded_categories = Embedding(input_dim=category_count, output_dim=embedding_dim)(embedded_categories)\n",
    "    \n",
    "    encoded_categories = Bidirectional(LSTM(lstm_units, return_sequences=True))(embedded_categories)\n",
    "    \n",
    "    return Model(inputs=input_categories, outputs=encoded_categories, name='category_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add6ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Self attention Layer\n",
    "\n",
    "class CategoryConditionalSelfAttention(Layer): \n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(CategoryConditionalSelfAttention, self).__init__(**kwargs)\n",
    "        self.units = units \n",
    "    \n",
    "    def build(self, input_shape): \n",
    "        # Weights for token embeddings (ht and ht')\n",
    "        self.W1 = self.add_weight(name='W1', shape=(self.units, input_shape[0][-1]), initializer='random_normal')\n",
    "        self.W2 = self.add_weight(name='W2', shape=(self.units, input_shape[0][-1], initializer='random_normal')\n",
    "        # Weight for category embedding (ec)\n",
    "        self.W3 = self.add_weight(name='W3', shape=(self.units, input_shape[1][-1]), initializer='random_normal')\n",
    "        \n",
    "        # Bias terms \n",
    "        self.b_g = self.add_weight(name='b_g', shape=(self.units,), initializer='zeros')\n",
    "        self.b_alpha = self.add_weight(name='b_alpha', shape=(1,), initializer='zeros')\n",
    "        \n",
    "        # Attention Weights \n",
    "        self.w_alpha = self.add_weight(name='w_alpha', shape=self.units, 1), initializer='random_normal')\n",
    "        \n",
    "        super(CategoryConditionalSelfAttention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        token_embeddings, category_embedding = inputs \n",
    "        \n",
    "        ## Adding an extra dimension \n",
    "        category_embedding = K.expand_dims(category_embedding, axis=1)\n",
    "        \n",
    "        ## Calculating scores\n",
    "        score_first_part = K.dot(token_embeddings, self.W1)\n",
    "        score_second_part = K.dot(token_embeddings, self.W2)\n",
    "        score_third_part = K.dot(category_embedding, self.W3)\n",
    "        \n",
    "        scores = K.tanh(score_first_part[:, None, :] + score_second_part + score_third_part + self.b_g)\n",
    "        \n",
    "        ## Attention weights\n",
    "        attention_weights = K.sigmoid(K.dot(scores, self.w_alpha) + self.b_alpha)\n",
    "        attention_weights = K.squeeze(attention_weights, -1)\n",
    "        \n",
    "        ## Contextualized token embeddings\n",
    "        context_embeddings = K.sum(attention_weights[:, :, None] * token_embeddings[:, None, :], axis=2)\n",
    "        \n",
    "        return context_embeddings\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CRF Layer\n",
    "\n",
    "contextualized_tokens = # previous layer's output\n",
    "\n",
    "num_tags = 4 * number_of_attributes +1 ## Replace with the number of attributes\n",
    "\n",
    "# CRF layer \n",
    "crf = CRF(num_tags)\n",
    "output, _ = crf(contextualized_tokens)\n",
    "\n",
    "#Build the model \n",
    "\n",
    "model = Model(inputs=your_input_layers, output=output)\n",
    "\n",
    "#Compile the model with the CRF loss and a suitable optimizer \n",
    "model.compile(optimizer='adam', loss=crf.loss, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preproccesing Data\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data['title'] + ' ' + data['description'])\n",
    "\n",
    "title_sequences = tokenizer.texts_to_sequences(data['title'])\n",
    "description_sequences = tokenizer.texts_to_sequences(data['description'])\n",
    "\n",
    "# Padding sequences to ensure uniform length\n",
    "\n",
    "max_length = 1000\n",
    "title_padded = pad_sequences(title_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "description_padded = pad_sequences(description_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split dataset \n",
    "\n",
    "train_titles, test_titles, train_descriptions, test_descriptions = train_test_split(\n",
    "    title_padded, description_padded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_your_model() #replace with your model creation function also train_labels and test labels.\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "##Train the model \n",
    "\n",
    "history = model.fit([train_tirles, train_decriptions], train_labels, epochs=10, validation_data=([test_titles, test_descriptions], test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
